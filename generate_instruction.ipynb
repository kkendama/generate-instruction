{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757d2697-97dd-405b-9a35-e6b50ab3396d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export FORCE_CMAKE=\"1\" \n",
    "!export LLAMA_CUBLAS=\"1\"\n",
    "!export CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87593e4c-5a1f-4f7d-aa5b-922b6783e246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824206aab0e44f909ebfc2a87b410968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)chatvector-Mixtral-instruct-v2-q6_K.gguf:   0%|          | 0.00/38.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"mmnga/aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-gguf\", filename=\"aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-q6_K.gguf\", local_dir=\"/notebooks/model_cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cccd67f-b1f2-46da-b184-0fcbb022af6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python --prefer-binary --upgrade --force-reinstall --no-cache-dir --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e27d03-0889-4829-8870-dd1409410a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ce2d81e72c4b908159c1ffb1efe6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/850 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cca4a54c0e74bb0b75b760effd9c931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d8b6aacdf24da2954320934c4d41c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a9df55078740bdb10546481a6c5932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcefdb0ab2c54fcc86d68b02c7d46c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('hpprc/jawiki-news', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ffd173-5ea2-4f8a-a7ee-fae2dbfc3e67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   yes\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: no\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 995 tensors from /notebooks/model_cards/aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  11:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q6_K:  834 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 35.74 GiB (6.57 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.38 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  102.92 MiB\n",
      "llm_load_tensors: VRAM used           = 36497.55 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8096\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1124/1124\n",
      "llama_new_context_with_model: compute buffer total size = 577.04 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 573.85 MiB\n",
      "llama_new_context_with_model: total VRAM used: 37071.40 MiB (model: 36497.55 MiB, context: 573.85 MiB)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# LLMの準備\n",
    "llm = Llama(model_path=\"/notebooks/model_cards/aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-q6_K.gguf\", n_ctx=8096, n_threads=1, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ebfcbb-7f3c-4907-b98a-1c06f12fcc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "users = {\n",
    "    0: \"小学生です。どんなことに対しても「なんでそうなるのか」が気になります。\",\n",
    "    1: \"この分野を専攻している大学生です。専門的な知識を身に着けようとしています。\",\n",
    "    2: \"社会人です。実務として使える知識・技術を欲しています。\",\n",
    "    3: \"この分野を専門としている大学教授です。自身の研究に役立つ知識を必要としています。\",\n",
    "    4: \"数学者です。どんなことに対しても、物事の原理・原則が気になります。\",\n",
    "    5: \"気分屋です。突然興味を持つ対象が変わることがあります。\",\n",
    "    6: \"意地悪な人です。AIアシスタントに対して間違った質問をします。\",\n",
    "    7: \"データサイエンティストです。データ分析のために必要な情報を整理しようとしています。\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e5610d-2465-4939-a65e-1811f1ff26fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d0102a-c673-4a03-af49-cfe07f549cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(title, text, user):\n",
    "    user_prompt = f'''<TITLE>\n",
    "{title}\n",
    "</TITLE>\n",
    "<TEXT>\n",
    "{text}\n",
    "</TEXT>\n",
    "\n",
    "上記のテキストを参考に、AIアシスタントへ問いかけを行うUSERと返答するASSISTANTの会話を作成してください。\n",
    "USERは{user}\n",
    "USERはまず指示の前提となる情報を提供します。\n",
    "USERは自分が満足するまでASSISTANTに対して指示を行います。\n",
    "USERはASSISTANTの応答に対して返答を行います。\n",
    "ASSISTANTは優秀なAIアシスタントで、USERからの指示に応えようとします。\n",
    "ASSISTANTは「です・ます」調で応答します。\n",
    "ASSISTANTはUSERからの指示に対して、聞かれたこと以外にもUSERが欲していそうな情報を付け加えようとします。\n",
    "<FORMAT>\n",
    "USER: \n",
    "ASSISTANT: \n",
    "USER: \n",
    "ASSISTANT: \n",
    "USER: \n",
    "ASSISTANT: \n",
    "</FORMAT>'''\n",
    "\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "    prompt = \"{b_inst} {system}{prompt} {e_inst} USER: \".format(\n",
    "        b_inst=B_INST,\n",
    "        system=f\"{B_SYS}あなたは優秀な日本人のAIアシスタントです。ユーザーからの指示に対して正確に回答してください。回答はMarkdown形式で行ってください。{E_SYS}\",\n",
    "        prompt=user_prompt,\n",
    "        e_inst=E_INST,\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ac0f71-685d-464b-ab42-a789e60d5e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred(prompt): # 推論の実行\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        temperature=0.8,\n",
    "        max_tokens=8096,\n",
    "        echo=True,\n",
    "    )\n",
    "    return output[\"choices\"][0][\"text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "844079a7-6bea-4fc2-be85-672d413b04f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#datasetの30000～40000までをdataframeに変換\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "output_file_path = \"wikinews-multi-turn-v1.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111124b-5a9a-42b5-9ca5-7471117b85f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0...\n",
      "鳥インフルエンザの感染が広がっているようですね。このウイルスはどのように人に感染するのでしょうか？\n",
      "ASSISTANT: 鳥インフルエンザウイルスは、通常、鶏やアヒルなどの鳥類に感染します。人への感染は、感染した鳥との接触や、感染した鳥の排せつ物などに触れることで起こると考えられています。しかし、人から人への感染は非常にまれであり、主に感染した家禽（鶏やアヒルなど）を処理する作業者に限られるとされています。\n",
      "USER: なるほど、鳥から人への感染が一般的だと。では、なぜインドネシア政府はワクチン播種を選んだのでしょう？\n",
      "ASSISTANT: インドネシア政府がワクチン播種を選んだ理由は、養鶏場経営者への経済補償の問題にあるとされています。大規模なニワトリの処分を行うと、養鶏場経営者に多額の補償が必要となります。そのため、インドネシア政府はワクチン播種を選んだと考えられます。しかし、ワクチン播種は感染を完全に防げるわけではありません。鳥インフルエンザウイルスが変異し、人への感染力が高まる可能性があるため、世界保健機関(WHO)は引き続き警戒を呼びかけています。\n",
      "USER: ワクチン播種によって、ウイルスが変異する可能性があるのですか？\n",
      "ASSISTANT: はい、その可能性があります。ワクチンを接種した鶏の体内で、ウイルスが変異することがあります。変異したウイルスが人に感染しやすくなったり、重症化させたりする可能性があるため、ワクチン播種は慎重に行う必要があります。世界保健機関(WHO)では、鳥インフルエンザの感染拡大を防ぐために、養鶏場でのニワトリの大規模な処分を勧告しています。\n",
      "USER: ワクチン播種とウイルスの変異、それぞれのメリットとデメリットを教えてください。\n",
      "ASSISTANT: ワクチン播種のメリットは、鶏の大規模な処分を行う必要がなく、経済的負担を軽減できる点にあります。一方、デメリットはウイルスの変異リスクです。ワクチン接種後も感染しないわけではなく、感染した鶏から排せつされるウイルスが変異する可能性があります。\n",
      "大規模な処分のメリットは、ウイルスを根絶できる点にあります。デメリットは、養鶏場経営者への補償問題や、食料供給への影響などが挙げられます。\n",
      "USER: ワクチン播種と大規模処分、それぞれについて最悪のシナリオを教えてください。\n",
      "ASSISTANT: ワクチン播種の最悪のシナリオは、ウイルスが変異し人への感染力が高まったり、重症化させるようになることです。その場合、世界的な大流行（パンデミック）を引き起こす危険性があります。\n",
      "一方、大規模処分の最悪のシナリオは、食料供給への影響や補償問題が深刻化し、養鶏場経営者が破産に追い込まれることです。さらに、ウイルスを完全に根絶できなかった場合、再び感染拡大のリスクが生じます。\n",
      "Processing 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     310.92 ms /  1309 runs   (    0.24 ms per token,  4210.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6615.76 ms /   937 tokens (    7.06 ms per token,   141.63 tokens per second)\n",
      "llama_print_timings:        eval time =  115887.63 ms /  1308 runs   (   88.60 ms per token,    11.29 tokens per second)\n",
      "llama_print_timings:       total time =  127114.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この試合で、駒大苫小牧はどのようにして4点を獲得したのですか？\n",
      "ASSISTANT: 1回裏に、本間選手の三塁打と相手の失策により2点を先制しました。その後、山口選手の2点適時打でさらに2点を加え、4点を獲得したのです。\n",
      "USER: その後、智辯和歌山は1点を返していますが、どのようにして1点を獲得したのでしょう？\n",
      "ASSISTANT: 2回表、古宮選手の2点適時打で1点を返しました。この試合では両チームともに走者を出す場面が多く、得点機会が何度かあったのですが、その中の一つです。\n",
      "USER: 駒大苫小牧が3回表に2点追加しましたが、どのようにしてこの2点を獲得したのでしょう？\n",
      "ASSISTANT: 3回裏に山口選手がタイムリー二塁打を放ち、2点を追加しました。これにより、駒大苫小牧は試合を有利に進めることができたのです。\n",
      "USER: なぜ8回表に早実が1点追加できたのですか？ \n",
      "ASSISTANT: 8回裏、斎藤選手が適時二塁安打を放ち、1点を獲得しました。これにより、早実は試合の流れをつかみ、完封勝ちをおさめることができたのです。\n",
      "Processing 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     127.62 ms /   486 runs   (    0.26 ms per token,  3808.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11519.95 ms /  1376 tokens (    8.37 ms per token,   119.44 tokens per second)\n",
      "llama_print_timings:        eval time =   48838.37 ms /   485 runs   (  100.70 ms per token,     9.93 tokens per second)\n",
      "llama_print_timings:       total time =   61722.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     173.05 ms /   692 runs   (    0.25 ms per token,  3998.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6373.37 ms /   880 tokens (    7.24 ms per token,   138.07 tokens per second)\n",
      "llama_print_timings:        eval time =   55665.05 ms /   691 runs   (   80.56 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =   63982.49 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "福本潤一参議院議員が離党したことで、7月の参院選への出馬はどうなるのでしょうか？\n",
      "ASSISTANT: 福本議員は、7月の参院選に野党から出馬を希望しています。ただ、民主党幹部は集票が期待できず、党内外からの反発が予想されることから、福本議員を擁立する意向は低いようです。\n",
      "<br>\n",
      "USER: そもそもなぜ福本議員は離党に至ったのでしょう？\n",
      "ASSISTANT: 福本議員が離党を決意した理由は、今年7月の参院選で公明党から公認されなかったことに対する不満が背景にあると考えられます。読売新聞によると、福本議員は1995年に新進党から出馬し、初当選。その後、新進党の分裂で公明党に移り、現在2期目です。\n",
      "<br>\n",
      "USER: 福本議員の離党が与党間の参院選対策にどのような影響をもたらすのでしょうか？\n",
      "ASSISTANT: 福本議員の離党は、与党間の参院選対策に波紋を広げる可能性があります。通常、与党議員が単独で離党して野党から出馬することは異例であり、政界全体においても大きな注目を浴びています。\n",
      "<br>\n",
      "USER: 公明党は福本議員の離党届を受理しないと発表しましたが、除名処分の背景について教えてください。\n",
      "ASSISTANT: 公明党の北側一雄幹事長は、福本議員の離党届を受理せず、除名処分を下す方針を示しました。これは、支持者への裏切り行為とみなされているためです。福本議員が公明党に所属していた間、どのような功績や影響をもたらしたかについては、具体的な情報がないため、コメントできません。\n",
      "<br>\n",
      "Processing 3...\n",
      "河合隼雄さんってどんな人？\n",
      "ASSISTANT: 河合隼雄さんは、臨床心理学者であり、文化庁長官や京都大学名誉教授を務めた方です。ユング派分析家としても知られています。\n",
      "USER: どうして有名になったの？\n",
      "ASSISTANT: 河合隼雄さんは、1982年に大佛次郎賞、1988年に新潮社学芸賞を受賞されました。その後、2000年に文化功労者に選出されています。これらの受賞や文化庁長官を務めた経験が、その名を高める一因となりました。\n",
      "USER: どうして文化庁長官になったの？\n",
      "ASSISTANT: 河合隼雄さんは京都大学や国際日本文化研究センターで教授や所長を歴任されました。これらの経験により、豊富な知識とリーダーシップが認められ、2002年に文化庁長官に就任しました。\n",
      "USER: どうして休職せざるをえなかったの？\n",
      "ASSISTANT: USERさん、河合隼雄さんは2006年8月に脳梗塞で倒れられました。この健康上の理由が原因で休職をせざるをえなくなり、翌年の任期満了を持って文化庁長官を退職されました。\n",
      "USER: 河合隼雄さんには兄弟がいるの？それとも一人っ子？\n",
      "ASSISTANT: USERさん、河合隼雄さんには実際にお兄様がいらっしゃいます。ニホンザル研究の第一人者であり、京都大学名誉教授の河合雅雄さんです。\n",
      "Processing 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     160.97 ms /   597 runs   (    0.27 ms per token,  3708.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5996.72 ms /   838 tokens (    7.16 ms per token,   139.74 tokens per second)\n",
      "llama_print_timings:        eval time =   48908.90 ms /   596 runs   (   82.06 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =   56684.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北陸新幹線の列車名が決まったみたいだね。最速列車の名前は何だろう？\n",
      "ASSISTANT: 最速列車である東京 - 金沢間の直通列車は、「かがやき」という名称です。これは1988年に長岡 - 金沢間を結ぶ上越新幹線連絡の在来線特急列車として登場しましたが、その後廃止されていました。今回18年ぶりに復活を果たしました。\n",
      "USER: へえ、かがやきってどういう意味なんだい？\n",
      "ASSISTANT: 「かがやき」は、輝く光がスピード感と明るい未来をイメージさせることから名付けられました。北陸新幹線で最速の列車としてふさわしいネーミングですね。\n",
      "USER: そうなんだ！他の列車にはどんな意味があるんだろう？\n",
      "ASSISTANT: 東京 - 金沢間の各駅に停車するタイプは、「はくたか」という名称です。これは1965年に東京 - 金沢間を結ぶ在来線特急列車として登場しましたが、その後廃止されました。その後、北越急行ほくほく線開業に伴い1997年に復活を遂げ、現在も運行されています。\n",
      "USER: あとにはどんな名前があるんだい？\n",
      "ASSISTANT: 富山 - 金沢間を結ぶシャトルタイプの列車は、「つるぎ」という名称が付けられました。これはかつて関西 - 北陸を結んだ列車として親しまれていた名前であり、20年ぶりの復活となります。\n",
      "USER: そうなんだ！「つるぎ」はどんな意味があるんだろう？\n",
      "ASSISTANT: 「つるぎ」には特別な意味は設定されていませんでした。ただ、1961年に大阪と北陸を結ぶ夜行準急列車として登場した歴史があり、多くの人に親しまれていたことから選ばれたようです。\n",
      "USER: なるほどね！もう一度「はくたか」について教えてください。\n",
      "ASSISTANT: 「はくたか」は、東京 - 金沢間の各駅に停車するタイプの列車です。1965年に東京 - 金沢間を結ぶ在来線特急として登場しましたが、その後廃止されました。その後、北越急行ほくほく線開業に伴い1997年に復活を遂げ現在も運行されています。首都圏と北陸をつなぐ列車として愛されている歴史ある名称です。\n",
      "Processing 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     230.75 ms /   911 runs   (    0.25 ms per token,  3947.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11516.61 ms /  1399 tokens (    8.23 ms per token,   121.48 tokens per second)\n",
      "llama_print_timings:        eval time =   95983.80 ms /   910 runs   (  105.48 ms per token,     9.48 tokens per second)\n",
      "llama_print_timings:       total time =  110308.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この事件について、もっと詳しく教えて。\n",
      "ASSISTANT: この事件は、2011年10月19日に滋賀県大津市の市立中学校で発生しました。臨時講師の岩佐秀人容疑者が、特別支援学級（難聴）に通う聴覚障害のある姉妹に対し、体罰を加えたとされるものです。\n",
      "USER: どのような被害があったの？\n",
      "ASSISTANT: 岩佐容疑者は2009年1月から2010年11月までの間、姉に対して頭を殴るなどの暴行を数回繰り返し、さらにボールペンの先端で脚などを突き、4日間の怪我を負わせました。妹に対しても、2010年6月から2011年5月までの間に、暴行を数回に亘り繰り返したとされています。\n",
      "USER: そんなひどいことが起きていたなんて。逮捕容疑について教えて。\n",
      "ASSISTANT: 岩佐秀人容疑者は、傷害と暴行の容疑で逮捕されました。この事件を受け、大津市教育委員会は2011年5月中旬に学校側が岩佐容疑者へ退職を勧告し、同年6月23日付で依願退職しています。\n",
      "USER: どうして体罰を加えるようになったの？\n",
      "ASSISTANT: 岩佐容疑者は「授業内容を自分の唇の動きで理解しないことに腹が立った」と供述しています。被害を受けた姉妹は手話や筆談による意思疎通は可能であったものの、岩佐容疑者の怒りがエスカレートしてしまったようです。\n",
      "Processing 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     146.48 ms /   631 runs   (    0.23 ms per token,  4307.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9581.05 ms /  1190 tokens (    8.05 ms per token,   124.20 tokens per second)\n",
      "llama_print_timings:        eval time =   54781.10 ms /   630 runs   (   86.95 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =   66039.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この白いんげん豆ダイエット法で、なぜ健康被害が起きてしまったのでしょう？\n",
      "ASSISTANT: 白いんげん豆に含まれる蛋白質の一種、レクチンという成分が、生であったり未加熱で摂取することで消化管への刺激作用を示し、下痢や嘔吐などの食中毒症状を引き起こします。\n",
      "USER: レクチンは加熱によって無害化されるものなのですか？\n",
      "ASSISTANT: そのとおりです。レクチンは、加熱することで熱変成を起こし、有害性がなくなります。通常、調理された白いんげん豆には問題ありません。\n",
      "USER: ファセオリンもレクチンの一種だと書かれていますが、こちらも加熱で無害化されるのでしょうか？\n",
      "ASSISTANT: はい、ファセオリンもレクチンの一種であり、同様に加熱によって有害性がなくなります。ただし、この記事で言及されているファセオラミンは、実際にはダイエット効果を示す根拠が乏しく、その点でも注意が必要です。\n",
      "USER: なぜこのような問題が起きてしまったのでしょう？\n",
      "ASSISTANT: 番組内では豆の生食を避けるよう呼びかけていましたが、視聴者に正確な調理方法が十分に伝わらなかった可能性や、注意点を見逃してしまった人がいたことが原因だと考えられます。\n",
      "USER: レクチンの問題は白いんげん豆特有のものですか？他の食材にも同様のリスクはあるのでしょうか？\n",
      "ASSISTANT: レクチンは、インゲンマメやレンズマメなどの豆類全般に含まれています。適切な調理を行えば問題ありませんが、生で食べることは避けるべきです。\n",
      "Processing 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     155.31 ms /   661 runs   (    0.23 ms per token,  4256.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8253.42 ms /  1058 tokens (    7.80 ms per token,   128.19 tokens per second)\n",
      "llama_print_timings:        eval time =   55339.58 ms /   660 runs   (   83.85 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   65401.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この歌詞は「#超絶かわいい」というタイトルですが、どんな意味が込められているんですか？\n",
      "ASSISTANT: こんにちは！この曲は、HoneyWorksさんの作品で、「#超絶かわいい」というタイトルには「非常に可愛い」という意味が込められています。歌っているmona役の夏川椎菜さんを\"超絶かわいい\"存在だとアピールする内容です。作詞・作曲・編曲はHoneyWorksが手がけています。\n",
      "USER: そうなんだ！でももっと具体的に、この歌詞はどんな状況を表してるんだろう？\n",
      "ASSISTANT: mona自身がファンに向けてアピールする楽曲です。SNSや動画サイトを通じて、monaのかわいさが伝われば嬉しいと願っています。「花嫁修業頑張るね」などの歌詞から、結婚を連想させるワードも見受けられますが、全体的には恋人気分や愛情を表現した歌詞と言えるでしょう。\n",
      "USER: なるほどね。じゃあこの曲で使われている「推し」ってどういう意味？\n",
      "ASSISTANT: \"推し\"は、アイドルやキャラクターなど自分が特に応援する対象を指します。元々は日本のオタク用語で、「(好きな)推しメン」や略して「推し」と言われます。インターネットスラングの1つで、SNSや掲示板等で使われるようになり、今では一般化された用語です。\n",
      "USER: そうだったんだ！\"推し\"って自分が応援する対象を指す言葉かー。面白いね！\n",
      "ASSISTANT: 日本語には、独特な表現やカルチャーがあり、「推し」もその1つですよね。これからも様々な用語や文化を楽しんで頂ければと思います！何か他に分からない言葉やトピックがありましたら、お気軽にお尋ねください。\n",
      "Processing 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     170.77 ms /   702 runs   (    0.24 ms per token,  4110.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15241.07 ms /  1664 tokens (    9.16 ms per token,   109.18 tokens per second)\n",
      "llama_print_timings:        eval time =   73697.24 ms /   701 runs   (  105.13 ms per token,     9.51 tokens per second)\n",
      "llama_print_timings:       total time =   90897.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007年から始まったクライマックス・シリーズはどのようにして決定されたのですか？\n",
      "ASSISTANT: 2007年から採用されたクライマックス・シリーズは、日本プロ野球組織によって行われた一般公募で「クライマックス・シリーズ」という名称が選ばれました。1,948件の応募の中から選ばれた大会名は、日本シリーズ出場権をかけた最後の攻防や、クライマックス（頂点、佳境）という意味が込められています。セ・パ両リーグで「クライマックス・セ」「クライマックス・パ」という大会名を使用しています。\n",
      "<NEXT>\n",
      "USER: その公募にはどのようなプロセスがあったのですか？\n",
      "ASSISTANT: 2006年11月15日、朝日新聞によると、クライマックス・シリーズの大会名称は一般公募され、そこで選ばれた名称が採用されました。\n",
      "</NEXT>\n",
      "USER: どういった意味や背景がある大会名なのですか？\n",
      "ASSISTANT: クライマックス・シリーズという名称には、日本シリーズ出場権をかけた最後の攻防や頂点、佳境などの意味が込められています。これは日本プロ野球組織が公募した1,948件の中から選ばれた名称であり、スポーツニッポンによるとセ・パ両リーグで「クライマックス・セ」「クライマックス・パ」という大会名を使用しています。\n",
      "</NEXT>\n",
      "USER: 他にどのような名称が候補に挙がっていたのですか？\n",
      "ASSISTANT: 残念ながら、一般公募された1,948件の中でどのような名称が候補として挙がっていたのかについては、具体的な情報を見つけることができませんでした。その中から選ばれた「クライマックス・シリーズ」という名称に日本プロ野球組織は頂点や佳境などの意味を込め、セ・パ両リーグで使用する大会名として採用されました。\n",
      "</NEXT>\n",
      "USER: クライマックス・シリーズは公募以外にどのような検討が行われたのですか？\n",
      "ASSISTANT: クライマックス・シリーズに関して、一般公募以外に報告されている具体的な検討プロセスは見当たりませんでした。1,948件の応募から選ばれた名称が日本プロ野球組織によって採用され、2007年から始まる新しいポストシーズントーナメントとなりました。セ・パ両リーグで「クライマックス・セ」「クライマックス・パ」と呼ばれ、日本シリーズ出場権をかけた大会と位置づけられています。\n",
      "</NEXT>\n",
      "USER: どうもありがとうございました。この情報は私の研究に役立つでしょう。\n",
      "ASSISTANT: ユーザー様のお役に立てて光栄です！ クライマックス・シリーズがセ・パ両リーグの上位3チームによるポストシーズントーナメントとして選ばれた名称であり、日本シリーズ出場権をかけた重要な大会だということが分かりました。もし何か他にご質問やお探しの情報があれば、お気軽にお申し付けください。私、AIアシスタントが精一杯お手伝いいたします！\n",
      "Processing 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     313.49 ms /  1247 runs   (    0.25 ms per token,  3977.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5481.73 ms /   647 tokens (    8.47 ms per token,   118.03 tokens per second)\n",
      "llama_print_timings:        eval time =  103158.76 ms /  1246 runs   (   82.79 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =  112768.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "榎本喜八ってどんな選手だったの？\n",
      "ASSISTANT: 榎本喜八氏は、元プロ野球選手で「安打製造機」や「ミスターオリオンズ」と呼ばれていました。東京都出身で、早稲田実業学校高等部在籍時代の1954年に甲子園大会に春・夏連続出場を果たされています。その後、毎日オリオンズ（現:千葉ロッテマリーンズ）に入団し、新人王や2度の首位打者など数多くのタイトルを獲得されています。通算で2314安打・打率2割9分8厘・246本塁打という素晴らしい成績を残されています。\n",
      "USER: そんなに活躍してたのに、引退後は野球界と距離を置いていたの？\n",
      "ASSISTANT: はい、榎本喜八氏は現役引退後、あまり野球界と関わらなくなったようです。名球会の会合にも出席されることはなかったそうです。打撃技術を徹底的に追求する性格だったと言われており、現役末期に座禅を組むなど独自の取り組みもされていました。\n",
      "USER: オリオンズで活躍した選手は他にもいる？\n",
      "ASSISTANT: 毎日・大毎・東京・ロッテと変化したオリオンズには、榎本喜八氏のほかにも多くの名選手が在籍されていました。例えば、「ミサイル打線」でおなじみの山内一弘氏や田宮謙次郎氏もオリオンズで活躍されていました。榎本喜八氏はそうしたスター選手たちと共に、「ミスターオリオンズ」と呼ばれる存在であったようです。\n",
      "<BR>\n",
      "USER: 安打製造機じゃなくて、ホームランバッターだったんじゃないの？\n",
      "ASSISTANT: 榎本喜八氏は確かに通算246本塁打という長距離打者としても優れた成績を残されています。しかし、安打製造機と呼ばれる由縁はその高い打率にあります。榎本氏の通算打率は2割9分8厘であり、これはプロ野球史上15位という驚くべき成績です。安打を量産したことから、「安打製造機」と呼ばれるようになったのです。\n",
      "Processing 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     207.56 ms /   818 runs   (    0.25 ms per token,  3941.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8522.08 ms /  1066 tokens (    7.99 ms per token,   125.09 tokens per second)\n",
      "llama_print_timings:        eval time =   74677.07 ms /   817 runs   (   91.40 ms per token,    10.94 tokens per second)\n",
      "llama_print_timings:       total time =   85711.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "福岡6区補選で自民党はどんな動きを見せているの？\n",
      "ASSISTANT: 福岡6区補選では、自民党福岡県連が公認申請している蔵内謙氏と、古屋圭司選対委員長が会談を行なったようです。その中で、鳩山邦夫元法務大臣の長男であり、既に出馬表明している鳩山二郎大川市長がリードしているという世論調査の結果を踏まえ、蔵内氏に対して出馬辞退を促しました。ただ、蔵内氏は立候補の意思を変えなかったと報じられています。自民党執行部では候補者を一本化したい考えがあるようですが、現在は2名の方が出馬の準備を進めている状況です。\n",
      "USER: 鳩山二郎さんってどんな人？\n",
      "ASSISTANT: 鳩山二郎大川市長は、鳩山邦夫元法務大臣の長男です。2014年に大川市長選挙に当選し、現在2期目を務めていらっしゃいます。父である鳩山邦夫元法務大臣は、自民党の重要ポストを歴任された政治家でした。鳩山二郎市長は、地域活性化に熱心に取り組んでおり、農業や文化、スポーツなど様々な分野で大川市の発展に貢献しています。\n",
      "USER: 他の政党はどうなってるの？\n",
      "ASSISTANT: 民進党は元在インド日本総領事館職員の新井富美子氏を擁立する予定です。共産党は筑後地区委員長の小林解子氏を支持しています。さらに、野党共闘の動きもあると報じられています。各政党とも、選挙戦でそれぞれの主張や政策をアピールし、有権者の支持を獲得しようとしている状況です。\n",
      "USER: なるほど、ありがとう。\n",
      "ASSISTANT: 選挙は、私たち国民に政治へ参加する機会でもあります。候補者や各政党の公約をしっかりと確認し、投票に向けて情報収集を行うことが大切です。ご不明な点などがあれば、いつでもお答えいたします。お気軽にお声がけください。\n",
      "Processing 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3219.00 ms\n",
      "llama_print_timings:      sample time =     217.58 ms /   807 runs   (    0.27 ms per token,  3709.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5671.88 ms /   798 tokens (    7.11 ms per token,   140.69 tokens per second)\n",
      "llama_print_timings:        eval time =   66855.54 ms /   806 runs   (   82.95 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =   75134.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Processing {k}...\")\n",
    "    user_id = random.randint(0, len(users)-1)\n",
    "    user = users[user_id]\n",
    "    text_id = random.randint(0,len(dataset)-1)\n",
    "    data = dataset[text_id]\n",
    "    text = data['text']\n",
    "    if len(text) > 5000:\n",
    "        text = text[:5000]\n",
    "    output = pred(generate_prompt(data['title'], text, user))\n",
    "    print(output)\n",
    "    result = {\"chat\": output, \"text_id\": data['id'], \"text_title\": data['title'], \"text\": text, \"user\": user}\n",
    "    with open(output_file_path, \"a\", encoding=\"utf-8\") as outfile:\n",
    "        json_line = json.dumps(result, ensure_ascii=False)\n",
    "        outfile.write(json_line + \"\\n\")\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139bf05-b02d-4775-ac83-c980c1b6be30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ccf5c-05c1-498a-8688-4c229862d49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
